{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **T√°c v·ª•: Hu·∫•n luy·ªán m√¥ h√¨nh ph√¢n v√πng t·ªïn th∆∞∆°ng ph·ªïi t·ª´ ·∫£nh X-quang ng·ª±c (Processed Dataset)**\n",
        "\n",
        "## **M√¥ t·∫£ b√†i to√°n**\n",
        "Ph√¢n v√πng v√πng t·ªïn th∆∞∆°ng tr√™n ·∫£nh X-quang l√† b√†i to√°n tr·ªçng y·∫øu trong ch·∫©n ƒëo√°n h√¨nh ·∫£nh y h·ªçc. Nhi·ªám v·ª• c·ªßa b·∫°n l√† hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh deep learning c√≥ kh·∫£ nƒÉng x√°c ƒë·ªãnh v√πng b·∫•t th∆∞·ªùng tr√™n ·∫£nh X-quang v√† minh ho·∫° v√πng ch√∫ √Ω b·∫±ng k·ªπ thu·∫≠t XAI (Explainable AI).\n",
        "\n",
        "## **Dataset (Processed Version)**\n",
        "B·ªô d·ªØ li·ªáu \"Chest X-ray Masks and Labels\" ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† t·ªëi ∆∞u h√≥a:\n",
        "- **283 ·∫£nh X-quang ng·ª±c** (t·ª´ 800 ·∫£nh g·ªëc)\n",
        "- **Train set**: 226 ·∫£nh (80%) - Chia th√†nh 203 train / 23 validation (90/10)\n",
        "- **Test set**: 57 ·∫£nh (20%) - ·∫®n ƒëi, kh√¥ng s·ª≠ d·ª•ng trong training\n",
        "- **K√≠ch th∆∞·ªõc ·∫£nh**: 256√ó256 pixels (resized t·ª´ 3000√ó2919)\n",
        "- **ƒê·ªãnh d·∫°ng**: PNG (·∫£nh RGB, mask grayscale)\n",
        "- **Class balance**: ~50/50 normal/tuberculosis\n",
        "\n",
        "## **C·∫•u tr√∫c Dataset**\n",
        "```\n",
        "chest-xray-masks-and-labels/\n",
        "‚îî‚îÄ‚îÄ Lung Segmentation/\n",
        "    ‚îú‚îÄ‚îÄ train/                 # 226 ·∫£nh (80%)\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ CXR_png/          # ·∫¢nh X-quang g·ªëc\n",
        "    ‚îÇ   ‚îú‚îÄ‚îÄ masks/            # Mask ph√¢n v√πng\n",
        "    ‚îÇ   ‚îî‚îÄ‚îÄ ClinicalReadings/ # Th√¥ng tin l√¢m s√†ng\n",
        "    ‚îî‚îÄ‚îÄ test/                 # 57 ·∫£nh (20%) - ·∫®n ƒëi\n",
        "        ‚îú‚îÄ‚îÄ CXR_png/\n",
        "        ‚îú‚îÄ‚îÄ masks/\n",
        "        ‚îî‚îÄ‚îÄ ClinicalReadings/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **1. C√†i ƒë·∫∑t v√† Import th∆∞ vi·ªán**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho Kaggle\n",
        "!pip install segmentation-models-pytorch==0.3.3\n",
        "!pip install albumentations==1.3.1\n",
        "!pip install captum==0.6.0\n",
        "!pip install opencv-python==4.8.1.78\n",
        "!pip install -U scipy==1.14.1\n",
        "!pip install numpy==1.26.4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Import cho XAI\n",
        "from captum.attr import IntegratedGradients\n",
        "from captum.attr._core.layer.grad_cam import LayerGradCam\n",
        "\n",
        "# Thi·∫øt l·∫≠p device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"S·ª≠ d·ª•ng device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2. C·∫•u h√¨nh v√† T·∫£i d·ªØ li·ªáu**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C·∫•u h√¨nh hyperparameters cho processed dataset\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 4  # Ph√π h·ª£p v·ªõi Kaggle GPU\n",
        "NUM_EPOCHS = 10  # Quick experiment v·ªõi processed dataset\n",
        "LEARNING_RATE = 0.001\n",
        "TRAIN_SPLIT = 0.9  # 90% train t·ª´ train folder\n",
        "VAL_SPLIT = 0.1    # 10% validation t·ª´ train folder\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu - t·ª± ƒë·ªông detect Kaggle environment\n",
        "if os.path.exists('/kaggle/input/chest-xray-masks-and-labels-processed'):\n",
        "    DATA_DIR = \"/kaggle/input/chest-xray-masks-and-labels-processed\"\n",
        "    print(\"S·ª≠ d·ª•ng processed dataset t·ª´ Kaggle input\")\n",
        "elif os.path.exists('/kaggle/input/chest-xray-masks-and-labels'):\n",
        "    DATA_DIR = \"/kaggle/input/chest-xray-masks-and-labels\"\n",
        "    print(\"S·ª≠ d·ª•ng original dataset t·ª´ Kaggle input\")\n",
        "else:\n",
        "    print(\"Kh√¥ng t√¨m th·∫•y dataset! Vui l√≤ng add processed dataset v√†o Kaggle notebook.\")\n",
        "    DATA_DIR = None\n",
        "\n",
        "if DATA_DIR:\n",
        "    # ƒê∆∞·ªùng d·∫´n ƒë·∫øn processed dataset structure\n",
        "    LUNG_SEG_DIR = os.path.join(DATA_DIR, \"Lung Segmentation\")\n",
        "    TRAIN_DIR = os.path.join(LUNG_SEG_DIR, \"train\")\n",
        "    \n",
        "    print(f\"C·∫•u h√¨nh:\")\n",
        "    print(f\"- Dataset path: {DATA_DIR}\")\n",
        "    print(f\"- Train directory: {TRAIN_DIR}\")\n",
        "    print(f\"- K√≠ch th∆∞·ªõc ·∫£nh: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
        "    print(f\"- Batch size: {BATCH_SIZE}\")\n",
        "    print(f\"- S·ªë epochs: {NUM_EPOCHS}\")\n",
        "    print(f\"- Learning rate: {LEARNING_RATE}\")\n",
        "    print(f\"- Train/Val split: {TRAIN_SPLIT}/{VAL_SPLIT}\")\n",
        "else:\n",
        "    print(\"Kh√¥ng th·ªÉ ti·∫øp t·ª•c v√¨ thi·∫øu dataset!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ki·ªÉm tra processed dataset structure\n",
        "if DATA_DIR and os.path.exists(TRAIN_DIR):\n",
        "    print(\"‚úÖ Processed dataset ƒë√£ ƒë∆∞·ª£c mount s·∫µn trong Kaggle environment!\")\n",
        "    print(f\"üìÅ Train directory: {TRAIN_DIR}\")\n",
        "    \n",
        "    # Ki·ªÉm tra c√°c th∆∞ m·ª•c con\n",
        "    train_images_dir = os.path.join(TRAIN_DIR, \"CXR_png\")\n",
        "    train_masks_dir = os.path.join(TRAIN_DIR, \"masks\")\n",
        "    \n",
        "    if os.path.exists(train_images_dir) and os.path.exists(train_masks_dir):\n",
        "        # ƒê·∫øm s·ªë ·∫£nh v√† mask trong train folder\n",
        "        train_images = [f for f in os.listdir(train_images_dir) if f.endswith('.png')]\n",
        "        train_masks = [f for f in os.listdir(train_masks_dir) if f.endswith('.png')]\n",
        "        \n",
        "        print(f\"üì∏ Train images: {len(train_images)}\")\n",
        "        print(f\"üé≠ Train masks: {len(train_masks)}\")\n",
        "        print(f\"‚úÖ Perfect alignment: {'Yes' if len(train_images) == len(train_masks) else 'No'}\")\n",
        "        \n",
        "        # T·∫°o th∆∞ m·ª•c output\n",
        "        os.makedirs('/kaggle/working/predictions', exist_ok=True)\n",
        "        os.makedirs('/kaggle/working/models', exist_ok=True)\n",
        "        os.makedirs('/kaggle/working/plots', exist_ok=True)\n",
        "        os.makedirs('/kaggle/working/gradcam', exist_ok=True)\n",
        "        print(\"üì¶ Th∆∞ m·ª•c output ƒë√£ ƒë∆∞·ª£c t·∫°o th√†nh c√¥ng!\")\n",
        "    else:\n",
        "        print(\"‚ùå Train folder structure kh√¥ng ƒë√∫ng!\")\n",
        "else:\n",
        "    print(\"‚ùå Processed dataset kh√¥ng t·ªìn t·∫°i!\")\n",
        "    print(\"H√£y ki·ªÉm tra l·∫°i:\")\n",
        "    print(\"1Ô∏è‚É£ Upload processed dataset 'chest-xray-masks-and-labels-processed' v√†o Kaggle.\")\n",
        "    print(\"2Ô∏è‚É£ Ho·∫∑c s·ª≠ d·ª•ng original dataset v√† modify code accordingly.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **3. Dataset v√† DataLoader (Modified for Processed Dataset)**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChestXrayDataset(Dataset):\n",
        "    \"\"\"Dataset cho ·∫£nh X-quang ng·ª±c v√† mask (Modified for processed dataset)\"\"\"\n",
        "\n",
        "    def __init__(self, image_files, train_dir, transform=None, is_training=True):\n",
        "        self.image_files = image_files\n",
        "        self.train_dir = train_dir\n",
        "        self.transform = transform\n",
        "        self.is_training = is_training\n",
        "        \n",
        "        # ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c images v√† masks trong train folder\n",
        "        self.images_dir = os.path.join(train_dir, \"CXR_png\")\n",
        "        self.masks_dir = os.path.join(train_dir, \"masks\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # ƒê∆∞·ªùng d·∫´n file ·∫£nh\n",
        "        image_filename = self.image_files[idx]\n",
        "        image_path = os.path.join(self.images_dir, image_filename)\n",
        "\n",
        "        # T·∫°o t√™n file mask t∆∞∆°ng ·ª©ng\n",
        "        mask_filename = image_filename.replace(\".png\", \"_mask.png\")\n",
        "        mask_path = os.path.join(self.masks_dir, mask_filename)\n",
        "\n",
        "        # ƒê·ªçc ·∫£nh v√† mask\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # N·∫øu mask kh√¥ng t·ªìn t·∫°i -> b√°o l·ªói r√µ r√†ng\n",
        "        if mask is None:\n",
        "            raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y mask t∆∞∆°ng ·ª©ng: {mask_path}\")\n",
        "\n",
        "        # Resize v·ªÅ k√≠ch th∆∞·ªõc chu·∫©n (processed dataset ƒë√£ ƒë∆∞·ª£c resize v·ªÅ 256x256)\n",
        "        image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "        mask = cv2.resize(mask, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "        # Chu·∫©n h√≥a mask v·ªÅ [0, 1]\n",
        "        mask = mask / 255.0\n",
        "\n",
        "        # √Åp d·ª•ng augmentations n·∫øu c√≥\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image, mask=mask)\n",
        "            image = augmented['image']\n",
        "            mask = augmented['mask']\n",
        "\n",
        "        # Chuy·ªÉn ƒë·ªïi sang tensor\n",
        "        image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
        "        mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
        "\n",
        "        return image, mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Augmentation cho processed dataset\n",
        "train_transform = A.Compose([\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
        "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
        "])\n",
        "\n",
        "val_transform = None  # Kh√¥ng augmentation cho validation\n",
        "\n",
        "print(\"‚úÖ Data augmentation ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p:\")\n",
        "print(\"- Training: Horizontal flip, rotation, brightness/contrast, noise\")\n",
        "print(\"- Validation: Kh√¥ng augmentation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L·∫•y to√†n b·ªô danh s√°ch ·∫£nh t·ª´ train folder\n",
        "train_images_dir = os.path.join(TRAIN_DIR, \"CXR_png\")\n",
        "all_images = sorted([f for f in os.listdir(train_images_dir) if f.endswith('.png')])\n",
        "\n",
        "# Gi·ªØ l·∫°i nh·ªØng ·∫£nh c√≥ mask t∆∞∆°ng ·ª©ng\n",
        "valid_images = []\n",
        "for img in all_images:\n",
        "    mask_path = os.path.join(TRAIN_DIR, \"masks\", img.replace('.png', '_mask.png'))\n",
        "    if os.path.exists(mask_path):\n",
        "        valid_images.append(img)\n",
        "\n",
        "print(f\"ü©ª T·ªïng s·ªë ·∫£nh c√≥ mask h·ª£p l·ªá trong train folder: {len(valid_images)} / {len(all_images)}\")\n",
        "\n",
        "# Chia train / validation t·ª´ processed dataset (90/10 split)\n",
        "train_files, val_files = train_test_split(valid_images, test_size=VAL_SPLIT, random_state=42)\n",
        "\n",
        "print(\"‚úÖ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c chia t·ª´ processed train folder:\")\n",
        "print(f\"- Train: {len(train_files)} ·∫£nh ({len(train_files)/len(valid_images)*100:.1f}%)\")\n",
        "print(f\"- Val:   {len(val_files)} ·∫£nh ({len(val_files)/len(valid_images)*100:.1f}%)\")\n",
        "print(f\"- Total processed: {len(valid_images)} ·∫£nh\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o DataLoader v·ªõi processed dataset\n",
        "train_dataset = ChestXrayDataset(train_files, TRAIN_DIR, transform=train_transform, is_training=True)\n",
        "val_dataset = ChestXrayDataset(val_files, TRAIN_DIR, transform=val_transform, is_training=False)\n",
        "\n",
        "# S·ª≠ d·ª•ng num_workers=0 cho Kaggle environment\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"DataLoader ƒë√£ ƒë∆∞·ª£c t·∫°o cho processed dataset:\")\n",
        "print(f\"- Train batches: {len(train_loader)}\")\n",
        "print(f\"- Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Ki·ªÉm tra m·ªôt batch d·ªØ li·ªáu\n",
        "sample_image, sample_mask = next(iter(train_loader))\n",
        "print(f\"Shape c·ªßa m·ªôt batch:\")\n",
        "print(f\"- Images: {sample_image.shape}\")\n",
        "print(f\"- Masks: {sample_mask.shape}\")\n",
        "print(f\"- Image range: [{sample_image.min():.3f}, {sample_image.max():.3f}]\")\n",
        "print(f\"- Mask range: [{sample_mask.min():.3f}, {sample_mask.max():.3f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **4. ƒê·ªãnh nghƒ©a m√¥ h√¨nh U-Net**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ƒê·ªãnh nghƒ©a m√¥ h√¨nh U-Net ƒë∆°n gi·∫£n (Same as original)\n",
        "class SimpleUNet(nn.Module):\n",
        "    \"\"\"U-Net architecture ƒë∆°n gi·∫£n cho segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, in_channels=3, out_channels=1):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.enc1 = self.conv_block(in_channels, 64)\n",
        "        self.enc2 = self.conv_block(64, 128)\n",
        "        self.enc3 = self.conv_block(128, 256)\n",
        "        self.enc4 = self.conv_block(256, 512)\n",
        "        \n",
        "        # Bottleneck\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "        \n",
        "        # Decoder\n",
        "        self.dec4 = self.conv_block(512 + 512, 512)\n",
        "        self.dec3 = self.conv_block(256 + 256, 256)\n",
        "        self.dec2 = self.conv_block(128 + 128, 128)\n",
        "        self.dec1 = self.conv_block(64 + 64, 64)\n",
        "        \n",
        "        # Final layer\n",
        "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "        # Pooling v√† upsampling\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.up = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
        "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
        "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
        "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "    \n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        \"\"\"Block convolution v·ªõi 2 l·ªõp conv\"\"\"\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(self.pool(enc1))\n",
        "        enc3 = self.enc3(self.pool(enc2))\n",
        "        enc4 = self.enc4(self.pool(enc3))\n",
        "        \n",
        "        # Bottleneck\n",
        "        bottleneck = self.bottleneck(self.pool(enc4))\n",
        "        \n",
        "        # Decoder v·ªõi skip connections\n",
        "        dec4 = self.up(bottleneck)\n",
        "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec4 = self.dec4(dec4)\n",
        "        \n",
        "        dec3 = self.up3(dec4)\n",
        "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
        "        dec3 = self.dec3(dec3)\n",
        "        \n",
        "        dec2 = self.up2(dec3)\n",
        "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
        "        dec2 = self.dec2(dec2)\n",
        "        \n",
        "        dec1 = self.up1(dec2)\n",
        "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
        "        dec1 = self.dec1(dec1)\n",
        "        \n",
        "        # Output\n",
        "        output = self.final(dec1)\n",
        "        output = self.sigmoid(output)\n",
        "        \n",
        "        return output\n",
        "\n",
        "# T·∫°o m√¥ h√¨nh\n",
        "model = SimpleUNet(in_channels=3, out_channels=1)\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"M√¥ h√¨nh U-Net ƒë√£ ƒë∆∞·ª£c t·∫°o v√† chuy·ªÉn ƒë·∫øn {device}\")\n",
        "print(f\"S·ªë parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **5. Loss Function v√† Optimizer**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ƒê·ªãnh nghƒ©a Dice Loss\n",
        "class DiceLoss(nn.Module):\n",
        "    \"\"\"Dice Loss cho segmentation\"\"\"\n",
        "    \n",
        "    def __init__(self, smooth=1.0):\n",
        "        super(DiceLoss, self).__init__()\n",
        "        self.smooth = smooth\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        # Flatten tensors\n",
        "        pred = pred.view(-1)\n",
        "        target = target.view(-1)\n",
        "        \n",
        "        # T√≠nh intersection v√† union\n",
        "        intersection = (pred * target).sum()\n",
        "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
        "        \n",
        "        return 1 - dice\n",
        "\n",
        "# ƒê·ªãnh nghƒ©a Combined Loss (Dice + BCE)\n",
        "class CombinedLoss(nn.Module):\n",
        "    \"\"\"K·∫øt h·ª£p Dice Loss v√† Binary Cross Entropy Loss\"\"\"\n",
        "    \n",
        "    def __init__(self, dice_weight=0.5, bce_weight=0.5):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.dice_loss = DiceLoss()\n",
        "        self.bce_loss = nn.BCELoss()\n",
        "        self.dice_weight = dice_weight\n",
        "        self.bce_weight = bce_weight\n",
        "    \n",
        "    def forward(self, pred, target):\n",
        "        dice = self.dice_loss(pred, target)\n",
        "        bce = self.bce_loss(pred, target)\n",
        "        return self.dice_weight * dice + self.bce_weight * bce\n",
        "\n",
        "# T·∫°o loss function v√† optimizer\n",
        "criterion = CombinedLoss(dice_weight=0.7, bce_weight=0.3)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)\n",
        "\n",
        "print(\"Loss function v√† optimizer ƒë√£ ƒë∆∞·ª£c thi·∫øt l·∫≠p:\")\n",
        "print(f\"- Loss: Combined Loss (Dice: 0.7, BCE: 0.3)\")\n",
        "print(f\"- Optimizer: Adam (lr={LEARNING_RATE})\")\n",
        "print(f\"- Scheduler: ReduceLROnPlateau (patience=3)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **6. H√†m t√≠nh metrics**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√†m t√≠nh c√°c metrics ƒë√°nh gi√°\n",
        "def calculate_dice(pred, target, threshold=0.5):\n",
        "    \"\"\"T√≠nh Dice Coefficient\"\"\"\n",
        "    pred_binary = (pred > threshold).float()\n",
        "    target_binary = target.float()\n",
        "    \n",
        "    intersection = (pred_binary * target_binary).sum()\n",
        "    union = pred_binary.sum() + target_binary.sum()\n",
        "    \n",
        "    dice = (2.0 * intersection) / (union + 1e-8)\n",
        "    return dice.item()\n",
        "\n",
        "def calculate_iou(pred, target, threshold=0.5):\n",
        "    \"\"\"T√≠nh Intersection over Union (IoU)\"\"\"\n",
        "    pred_binary = (pred > threshold).float()\n",
        "    target_binary = target.float()\n",
        "    \n",
        "    intersection = (pred_binary * target_binary).sum()\n",
        "    union = pred_binary.sum() + target_binary.sum() - intersection\n",
        "    \n",
        "    iou = intersection / (union + 1e-8)\n",
        "    return iou.item()\n",
        "\n",
        "def calculate_f1(pred, target, threshold=0.5):\n",
        "    \"\"\"T√≠nh F1 score cho segmentation (nh·ªã ph√¢n)\"\"\"\n",
        "    pred_binary = (pred > threshold).int().cpu().numpy().flatten()\n",
        "    target_binary = (target > threshold).int().cpu().numpy().flatten()\n",
        "    \n",
        "    # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p batch to√†n 0 ho·∫∑c to√†n 1 tr√°nh l·ªói sklearn\n",
        "    if len(np.unique(target_binary)) == 1:\n",
        "        return float(pred_binary.mean() == target_binary.mean())\n",
        "\n",
        "    return f1_score(target_binary, pred_binary, average='binary')\n",
        "\n",
        "def calculate_metrics(pred, target, threshold=0.5):\n",
        "    \"\"\"T√≠nh t·∫•t c·∫£ metrics\"\"\"\n",
        "    dice = calculate_dice(pred, target, threshold)\n",
        "    iou = calculate_iou(pred, target, threshold)\n",
        "    f1 = calculate_f1(pred, target, threshold)\n",
        "    \n",
        "    return {\n",
        "        'dice': dice,\n",
        "        'iou': iou,\n",
        "        'f1': f1\n",
        "    }\n",
        "\n",
        "print(\"C√°c h√†m t√≠nh metrics ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a:\")\n",
        "print(\"- calculate_dice(): Dice Coefficient\")\n",
        "print(\"- calculate_iou(): Intersection over Union\")\n",
        "print(\"- calculate_f1(): F1-Score\")\n",
        "print(\"- calculate_metrics(): T·∫•t c·∫£ metrics\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **7. Training Loop**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# H√†m training m·ªôt epoch\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Training m·ªôt epoch\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_dice = 0\n",
        "    total_iou = 0\n",
        "    total_f1 = 0\n",
        "    \n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "    \n",
        "    for batch_idx, (images, masks) in enumerate(progress_bar):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, masks)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # T√≠nh metrics\n",
        "        with torch.no_grad():\n",
        "            metrics = calculate_metrics(outputs, masks)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_dice += metrics['dice']\n",
        "        total_iou += metrics['iou']\n",
        "        total_f1 += metrics['f1']\n",
        "        \n",
        "        # C·∫≠p nh·∫≠t progress bar\n",
        "        progress_bar.set_postfix({\n",
        "            'Loss': f'{loss.item():.4f}',\n",
        "            'Dice': f'{metrics[\"dice\"]:.4f}'\n",
        "        })\n",
        "    \n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    avg_dice = total_dice / len(train_loader)\n",
        "    avg_iou = total_iou / len(train_loader)\n",
        "    avg_f1 = total_f1 / len(train_loader)\n",
        "    \n",
        "    return avg_loss, avg_dice, avg_iou, avg_f1\n",
        "\n",
        "# H√†m validation m·ªôt epoch\n",
        "def validate_epoch(model, val_loader, criterion, device):\n",
        "    \"\"\"Validation m·ªôt epoch\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_dice = 0\n",
        "    total_iou = 0\n",
        "    total_f1 = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        progress_bar = tqdm(val_loader, desc=\"Validation\")\n",
        "        \n",
        "        for images, masks in progress_bar:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            \n",
        "            # T√≠nh metrics\n",
        "            metrics = calculate_metrics(outputs, masks)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            total_dice += metrics['dice']\n",
        "            total_iou += metrics['iou']\n",
        "            total_f1 += metrics['f1']\n",
        "            \n",
        "            # C·∫≠p nh·∫≠t progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f'{loss.item():.4f}',\n",
        "                'Dice': f'{metrics[\"dice\"]:.4f}'\n",
        "            })\n",
        "    \n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    avg_dice = total_dice / len(val_loader)\n",
        "    avg_iou = total_iou / len(val_loader)\n",
        "    avg_f1 = total_f1 / len(val_loader)\n",
        "    \n",
        "    return avg_loss, avg_dice, avg_iou, avg_f1\n",
        "\n",
        "print(\"Training v√† validation functions ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, num_epochs):\n",
        "    \"\"\"Hu·∫•n luy·ªán m√¥ h√¨nh\"\"\"\n",
        "    \n",
        "    # L∆∞u tr·ªØ l·ªãch s·ª≠ training\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_dices = []\n",
        "    val_dices = []\n",
        "    train_ious = []\n",
        "    val_ious = []\n",
        "    train_f1s = []\n",
        "    val_f1s = []\n",
        "    \n",
        "    best_val_dice = 0\n",
        "    best_model_state = None\n",
        "    \n",
        "    print(\"B·∫Øt ƒë·∫ßu training v·ªõi processed dataset...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 40)\n",
        "        \n",
        "        # Training\n",
        "        train_loss, train_dice, train_iou, train_f1 = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, device\n",
        "        )\n",
        "        \n",
        "        # Validation\n",
        "        val_loss, val_dice, val_iou, val_f1 = validate_epoch(\n",
        "            model, val_loader, criterion, device\n",
        "        )\n",
        "        \n",
        "        # C·∫≠p nh·∫≠t learning rate\n",
        "        scheduler.step(val_loss)\n",
        "        \n",
        "        # L∆∞u tr·ªØ metrics\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_dices.append(train_dice)\n",
        "        val_dices.append(val_dice)\n",
        "        train_ious.append(train_iou)\n",
        "        val_ious.append(val_iou)\n",
        "        train_f1s.append(train_f1)\n",
        "        val_f1s.append(val_f1)\n",
        "        \n",
        "        # In k·∫øt qu·∫£\n",
        "        print(f\"Train - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}, IoU: {train_iou:.4f}, F1: {train_f1:.4f}\")\n",
        "        print(f\"Val   - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}, IoU: {val_iou:.4f}, F1: {val_f1:.4f}\")\n",
        "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "        \n",
        "        # L∆∞u best model\n",
        "        if val_dice > best_val_dice:\n",
        "            best_val_dice = val_dice\n",
        "            best_model_state = model.state_dict().copy()\n",
        "            print(f\"‚úì New best validation Dice: {best_val_dice:.4f}\")\n",
        "        \n",
        "        print(\"=\" * 60)\n",
        "    \n",
        "    # Load best model\n",
        "    if best_model_state is not None:\n",
        "        model.load_state_dict(best_model_state)\n",
        "        print(f\"ƒê√£ load best model v·ªõi validation Dice: {best_val_dice:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses,\n",
        "        'train_dices': train_dices,\n",
        "        'val_dices': val_dices,\n",
        "        'train_ious': train_ious,\n",
        "        'val_ious': val_ious,\n",
        "        'train_f1s': train_f1s,\n",
        "        'val_f1s': val_f1s,\n",
        "        'best_val_dice': best_val_dice\n",
        "    }\n",
        "\n",
        "print(\"Training function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# B·∫Øt ƒë·∫ßu training v·ªõi processed dataset\n",
        "print(\"B·∫Øt ƒë·∫ßu training v·ªõi processed dataset...\")\n",
        "print(f\"S·ªë epochs: {NUM_EPOCHS}, Batch size: {BATCH_SIZE}\")\n",
        "print(f\"Train samples: {len(train_files)}, Val samples: {len(val_files)}\")\n",
        "\n",
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    num_epochs=NUM_EPOCHS\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **8. Visualization Training History**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V·∫Ω bi·ªÉu ƒë·ªì training history\n",
        "def plot_training_history(history):\n",
        "    \"\"\"V·∫Ω bi·ªÉu ƒë·ªì l·ªãch s·ª≠ training\"\"\"\n",
        "    \n",
        "    epochs = range(1, len(history['train_losses']) + 1)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Loss\n",
        "    axes[0, 0].plot(epochs, history['train_losses'], 'b-', label='Train Loss')\n",
        "    axes[0, 0].plot(epochs, history['val_losses'], 'r-', label='Validation Loss')\n",
        "    axes[0, 0].set_title('Training v√† Validation Loss')\n",
        "    axes[0, 0].set_xlabel('Epochs')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True)\n",
        "    \n",
        "    # Dice Score\n",
        "    axes[0, 1].plot(epochs, history['train_dices'], 'b-', label='Train Dice')\n",
        "    axes[0, 1].plot(epochs, history['val_dices'], 'r-', label='Validation Dice')\n",
        "    axes[0, 1].set_title('Training v√† Validation Dice Score')\n",
        "    axes[0, 1].set_xlabel('Epochs')\n",
        "    axes[0, 1].set_ylabel('Dice Score')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True)\n",
        "    \n",
        "    # IoU\n",
        "    axes[1, 0].plot(epochs, history['train_ious'], 'b-', label='Train IoU')\n",
        "    axes[1, 0].plot(epochs, history['val_ious'], 'r-', label='Validation IoU')\n",
        "    axes[1, 0].set_title('Training v√† Validation IoU')\n",
        "    axes[1, 0].set_xlabel('Epochs')\n",
        "    axes[1, 0].set_ylabel('IoU')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True)\n",
        "    \n",
        "    # F1 Score\n",
        "    axes[1, 1].plot(epochs, history['train_f1s'], 'b-', label='Train F1')\n",
        "    axes[1, 1].plot(epochs, history['val_f1s'], 'r-', label='Validation F1')\n",
        "    axes[1, 1].set_title('Training v√† Validation F1 Score')\n",
        "    axes[1, 1].set_xlabel('Epochs')\n",
        "    axes[1, 1].set_ylabel('F1 Score')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # L∆∞u bi·ªÉu ƒë·ªì\n",
        "    plt.savefig('/kaggle/working/plots/training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # In k·∫øt qu·∫£ cu·ªëi c√πng\n",
        "    print(\"K·∫øt qu·∫£ training cu·ªëi c√πng:\")\n",
        "    print(f\"Best Validation Dice: {history['best_val_dice']:.4f}\")\n",
        "    print(f\"Final Train Dice: {history['train_dices'][-1]:.4f}\")\n",
        "    print(f\"Final Validation Dice: {history['val_dices'][-1]:.4f}\")\n",
        "\n",
        "# V·∫Ω bi·ªÉu ƒë·ªì\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **9. XAI - Explainable AI v·ªõi GradCAM**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple GradCAM Implementation (No hooks, reliable)\n",
        "def create_gradcam(model, input_tensor):\n",
        "    \"\"\"\n",
        "    Create GradCAM using input gradients only\n",
        "    This approach avoids all hook-related issues\n",
        "    \"\"\"\n",
        "    device = input_tensor.device\n",
        "    \n",
        "    # Enable gradients for input\n",
        "    input_tensor.requires_grad_(True)\n",
        "    \n",
        "    # Forward pass\n",
        "    output = model(input_tensor)\n",
        "    \n",
        "    # Use mean of output as target (for segmentation)\n",
        "    target = output.mean()\n",
        "    \n",
        "    # Backward pass\n",
        "    target.backward()\n",
        "    \n",
        "    # Get gradients w.r.t input\n",
        "    gradients = input_tensor.grad\n",
        "    \n",
        "    # Global average pooling of gradients\n",
        "    weights = gradients.mean(dim=(2, 3), keepdim=True)\n",
        "    \n",
        "    # Generate CAM by multiplying gradients with input\n",
        "    cam = (gradients * weights).sum(dim=1, keepdim=True)\n",
        "    \n",
        "    # Apply ReLU and normalize\n",
        "    cam = torch.relu(cam)\n",
        "    if cam.max() > 0:\n",
        "        cam = cam / cam.max()\n",
        "    \n",
        "    return cam.squeeze().detach().cpu().numpy()\n",
        "\n",
        "print(\"‚úÖ Simple GradCAM function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced GradCAM Visualization with Better Contrast\n",
        "def visualize_gradcam_enhanced(model, val_loader, device, num_samples=3, save_dir=None):\n",
        "    \"\"\"Enhanced GradCAM visualization with better contrast and visibility\"\"\"\n",
        "    model.eval()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    try:\n",
        "        # Get a batch\n",
        "        images, masks = next(iter(val_loader))\n",
        "        images, masks = images.to(device), masks.to(device)\n",
        "\n",
        "        # Predictions\n",
        "        with torch.no_grad():\n",
        "            predictions = model(images)\n",
        "\n",
        "        # Move to CPU\n",
        "        images_cpu = images.cpu()\n",
        "        masks_cpu = masks.cpu()\n",
        "        preds_cpu = predictions.cpu()\n",
        "\n",
        "        # Select random samples\n",
        "        indices = random.sample(range(len(images)), min(num_samples, len(images)))\n",
        "\n",
        "        # Create figure\n",
        "        fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            try:\n",
        "                img = images_cpu[idx].permute(1, 2, 0).numpy()\n",
        "                gt_mask = masks_cpu[idx].squeeze().numpy()\n",
        "                pred_mask = preds_cpu[idx].squeeze().numpy()\n",
        "                pred_binary = (pred_mask > 0.5).astype(np.uint8)\n",
        "\n",
        "                # Generate GradCAM\n",
        "                input_tensor = images[idx:idx+1].to(device)\n",
        "                cam = create_gradcam(model, input_tensor)\n",
        "                \n",
        "                # Enhanced normalization with better contrast\n",
        "                if cam.max() > cam.min():\n",
        "                    # Use percentile-based normalization for better contrast\n",
        "                    cam_min = np.percentile(cam, 5)  # Use 5th percentile instead of min\n",
        "                    cam_max = np.percentile(cam, 95)  # Use 95th percentile instead of max\n",
        "                    cam_norm = np.clip((cam - cam_min) / (cam_max - cam_min + 1e-8), 0, 1)\n",
        "                else:\n",
        "                    cam_norm = np.zeros_like(cam)\n",
        "\n",
        "                # Apply gamma correction for better visibility\n",
        "                cam_norm = np.power(cam_norm, 0.8)  # Gamma correction\n",
        "\n",
        "                # Create enhanced overlay with better colormap\n",
        "                # Use 'hot' colormap which is brighter and more visible\n",
        "                heatmap = plt.cm.hot(cam_norm)[:, :, :3]\n",
        "                \n",
        "                # Better overlay ratio - more heatmap, less original image\n",
        "                overlay_cam = 0.4 * img + 0.6 * heatmap\n",
        "                overlay_cam = np.clip(overlay_cam, 0, 1)\n",
        "\n",
        "                # Calculate Dice\n",
        "                dice = calculate_dice(preds_cpu[idx:idx+1], masks_cpu[idx:idx+1])\n",
        "\n",
        "                # Display\n",
        "                col_titles = [\n",
        "                    f\"·∫¢nh g·ªëc\\nDice={dice:.3f}\",\n",
        "                    \"Ground Truth\",\n",
        "                    \"Prediction\",\n",
        "                    \"GradCAM Heatmap\",\n",
        "                    \"Overlay (Enhanced)\"\n",
        "                ]\n",
        "\n",
        "                for j, data in enumerate([img, gt_mask, pred_mask, cam_norm, overlay_cam]):\n",
        "                    ax = axes[i, j] if num_samples > 1 else axes[j]\n",
        "                    if j in [1, 2]:  # grayscale for GT and prediction\n",
        "                        ax.imshow(data, cmap='gray')\n",
        "                    elif j == 3:  # GradCAM heatmap with hot colormap\n",
        "                        ax.imshow(data, cmap='hot')\n",
        "                    else:  # RGB images\n",
        "                        ax.imshow(data)\n",
        "                    ax.set_title(col_titles[j])\n",
        "                    ax.axis('off')\n",
        "\n",
        "                # Save individual images\n",
        "                if save_dir is not None:\n",
        "                    os.makedirs(save_dir, exist_ok=True)\n",
        "                    save_path = os.path.join(save_dir, f\"gradcam_enhanced_sample_{i+1}.png\")\n",
        "                    plt.imsave(save_path, overlay_cam)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample {i+1}: {e}\")\n",
        "                continue\n",
        "\n",
        "        plt.tight_layout()\n",
        "        \n",
        "        if save_dir:\n",
        "            plt.savefig(f'{save_dir}/gradcam_enhanced_visualizations.png', dpi=300, bbox_inches='tight')\n",
        "            print(f\"üìÅ ƒê√£ l∆∞u {len(indices)} ·∫£nh GradCAM enhanced t·∫°i: {save_dir}\")\n",
        "        \n",
        "        plt.show()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in enhanced GradCAM visualization: {e}\")\n",
        "\n",
        "print(\"‚úÖ Enhanced GradCAM visualization function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute GradCAM visualization\n",
        "print(\"üéØ Generating GradCAM visualizations...\")\n",
        "visualize_gradcam_enhanced(model, val_loader, device, num_samples=3, save_dir='/kaggle/working/gradcam')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **10. L∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# L∆∞u m√¥ h√¨nh ƒë√£ hu·∫•n luy·ªán\n",
        "output_dir = '/kaggle/working/models'\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(output_dir, 'unet_processed_dataset_model.pth')\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'training_history': history,\n",
        "    'model_config': {\n",
        "        'in_channels': 3,\n",
        "        'out_channels': 1,\n",
        "        'image_size': IMAGE_SIZE,\n",
        "        'dataset_type': 'processed',\n",
        "        'train_samples': len(train_files),\n",
        "        'val_samples': len(val_files)\n",
        "    }\n",
        "}, model_path)\n",
        "\n",
        "print(f\"M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o '{model_path}'\")\n",
        "\n",
        "# In t√≥m t·∫Øt k·∫øt qu·∫£ cu·ªëi c√πng\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"T√ìM T·∫ÆT K·∫æT QU·∫¢ CU·ªêI C√ôNG - PROCESSED DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Dataset: Processed Chest X-ray (283 images)\")\n",
        "print(f\"Train samples: {len(train_files)}\")\n",
        "print(f\"Validation samples: {len(val_files)}\")\n",
        "print(f\"Best Validation Dice: {history['best_val_dice']:.4f}\")\n",
        "print(f\"Final Train Dice: {history['train_dices'][-1]:.4f}\")\n",
        "print(f\"Final Validation Dice: {history['val_dices'][-1]:.4f}\")\n",
        "print(f\"Final Train IoU: {history['train_ious'][-1]:.4f}\")\n",
        "print(f\"Final Validation IoU: {history['val_ious'][-1]:.4f}\")\n",
        "print(f\"Final Train F1: {history['train_f1s'][-1]:.4f}\")\n",
        "print(f\"Final Validation F1: {history['val_f1s'][-1]:.4f}\")\n",
        "print(\"=\"*60)\n",
        "print(\"üìÅ Output files saved to:\")\n",
        "print(\"- /kaggle/working/models/unet_processed_dataset_model.pth\")\n",
        "print(\"- /kaggle/working/plots/training_history.png\")\n",
        "print(\"- /kaggle/working/gradcam/gradcam_enhanced_visualizations.png\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
